{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.scale as mscale\n",
    "import matplotlib.transforms as mtransforms\n",
    "\n",
    "from matplotlib.ticker import NullFormatter, ScalarFormatter, NullLocator, AutoLocator, MultipleLocator\n",
    "\n",
    "\n",
    "class SqrtScale(mscale.ScaleBase):\n",
    "    # The name under which this scale will be registered with matplotlib.\n",
    "    name = 'sqrt'\n",
    "\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        mscale.ScaleBase.__init__(self)\n",
    "\n",
    "    def get_transform(self):\n",
    "        return self.SqrtTransform()\n",
    "\n",
    "    def set_default_locators_and_formatters(self, axis):\n",
    "        axis.set_major_locator(AutoLocator())\n",
    "        axis.set_major_formatter(ScalarFormatter())\n",
    "        axis.set_minor_locator(NullLocator())\n",
    "        axis.set_minor_formatter(NullFormatter())\n",
    "\n",
    "    def limit_range_for_scale(self, vmin, vmax, minpos):\n",
    "        return max(vmin, 0.0), vmax\n",
    "\n",
    "    class SqrtTransform(mtransforms.Transform):\n",
    "        input_dims = 1\n",
    "        output_dims = 1\n",
    "        is_separable = True\n",
    "\n",
    "        def __init__(self):\n",
    "            mtransforms.Transform.__init__(self)\n",
    "\n",
    "        def transform_non_affine(self, a):\n",
    "            masked = np.ma.masked_where((a < 0.0), a)\n",
    "            if masked.mask.any():\n",
    "                return ma.sqrt(masked)\n",
    "            else:\n",
    "                return np.sqrt(a)\n",
    "\n",
    "        def inverted(self):\n",
    "            return SqrtScale.InvertedSqrtTransform()\n",
    "\n",
    "    class InvertedSqrtTransform(mtransforms.Transform):\n",
    "        input_dims = 1\n",
    "        output_dims = 1\n",
    "        is_separable = True\n",
    "\n",
    "        def __init__(self):\n",
    "            mtransforms.Transform.__init__(self)\n",
    "\n",
    "        def transform_non_affine(self, a):\n",
    "            return a**2\n",
    "\n",
    "        def inverted(self):\n",
    "            return SqrtScale.SqrtTransform()\n",
    "\n",
    "\n",
    "mscale.register_scale(SqrtScale)\n",
    "\n",
    "ranker2legend = {'CascadeUCB1[FC]': 'CUCB1-fc',\n",
    "                 'CascadeKL-UCB[FC]': 'CKL-UCB-fc',\n",
    "                 'RelativeCascadeUCB1Algorithm[FC]': 'UCT-fc',\n",
    "                 'QuickRank[FC]': 'QR-fc',\n",
    "                 'MergeRank[FC]': 'MR-fc',\n",
    "                 'MergeRankKL[FC]': 'MR-KL-fc',\n",
    "                 'MergeRankZeroKL[FC]': 'MR-Z-KL-fc',\n",
    "                 'ShuffleAndSplit[FC]': 'SnS-fc',\n",
    "                 'RankedBanditsUCB1[FC]': 'RB-UCB1-fc',\n",
    "                 'RankedBanditsExp3[FC]': 'RB-Exp3-fc',\n",
    "                 'RankedBanditsKL-UCB[FC]': 'RB-KL-fc',\n",
    "                 'RealMergeRankZeroKL[FC]': '#MR-ZE-KL-fc',\n",
    "\n",
    "                 'CascadeUCB1[LC]': 'CUCB1-lc',\n",
    "                 'CascadeKL-UCB[LC]': 'CKL-UCB-lc',\n",
    "                 'RelativeCascadeUCB1Algorithm[LC]': 'UCT-lc',\n",
    "                 'QuickRank[LC]': 'QR-lc',\n",
    "                 'MergeRank[LC]': 'MR-lc',\n",
    "                 'MergeRankKL[LC]': 'MR-KL-lc',\n",
    "                 'MergeRankZeroKL[LC]': 'MR-Z-KL-lc',\n",
    "                 'ShuffleAndSplit[LC]': 'SnS-lc',\n",
    "                 'RankedBanditsUCB1[LC]': 'RB-UCB1-lc',\n",
    "                 'RankedBanditsExp3[LC]': 'RB-Exp3-lc',\n",
    "                 'RankedBanditsKL-UCB[LC]': 'RB-KL-lc',\n",
    "                 'RealMergeRankZeroKL[LC]': '#MR-ZE-KL-lc',\n",
    "\n",
    "                 'CascadeUCB1[FF]': 'CUCB1-ff',\n",
    "                 'CascadeKL-UCB[FF]': 'CKL-UCB-ff',\n",
    "                 'RelativeCascadeUCB1Algorithm[FF]': 'UCT-ff',\n",
    "                 'QuickRank[FF]': 'QR-ff',\n",
    "                 'MergeRank[FF]': 'MR-ff',\n",
    "                 'MergeRankKL[FF]': 'MR-KL-ff',\n",
    "                 'MergeRankZeroKL[FF]': 'MR-Z-KL-ff',\n",
    "                 'ShuffleAndSplit[FF]': 'SnS-ff',\n",
    "                 'RankedBanditsUCB1[FF]': 'RB-UCB1-ff',\n",
    "                 'RankedBanditsExp3[FF]': 'RB-Exp3-ff',\n",
    "                 'RankedBanditsKL-UCB[FF]': 'RB-KL-ff',\n",
    "                 'RealMergeRankZeroKL[FF]': '#MR-ZE-KL-ff'}\n",
    "\n",
    "\n",
    "def plot_regret_curves(ax, info, regret, xscale=None, yscale=None, cumulative=False):\n",
    "    ranker_model_name = info['ranking_model'].getName()\n",
    "    click_model_name = info['click_model'].getName()\n",
    "    cutoff = info['cutoff']\n",
    "    qid = info['query']\n",
    "\n",
    "    if not cumulative:\n",
    "        regret = regret.cumsum()\n",
    "    \n",
    "    xscale = 'linear' if xscale is None else xscale\n",
    "    yscale = 'linear' if yscale is None else yscale\n",
    "    \n",
    "    # Subsample regret if there is more than 10^5 iterations.\n",
    "    if regret.shape[0] > 100000:\n",
    "        indices = np.linspace(0, regret.shape[0] - 1, 100000).astype('int32')\n",
    "        regret = regret[indices]\n",
    "    else:\n",
    "        indices = np.arange(regret.shape[0], dtype='int32')\n",
    "\n",
    "    ax.set_title('%s - %s@%d - Q%s' % (ranker_model_name, click_model_name, cutoff, qid))\n",
    "\n",
    "    ax.plot(indices, regret, ls='-', lw=1.5, color='k')\n",
    "\n",
    "    ax.set_ylabel('Regret - (CTR@%d)' % cutoff)\n",
    "    ax.set_xlabel('Impressions')\n",
    "\n",
    "    ax.set_xscale(xscale)\n",
    "    ax.set_yscale(yscale)\n",
    "\n",
    "    ax.title.set_fontsize(10)\n",
    "\n",
    "    ax.xaxis.label.set_fontsize(10)\n",
    "    ax.yaxis.label.set_fontsize(10)\n",
    "\n",
    "    ax.xaxis.get_offset_text().set_fontsize(10)\n",
    "    ax.yaxis.get_offset_text().set_fontsize(10)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=10)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=10)\n",
    "\n",
    "\n",
    "def plot_multiple_regret_curves(ax, infos, regrets, xscale=None, yscale=None, cumulative=False):\n",
    "    '''\n",
    "    Plots the regret curves for the experiments described in `infos` with axis\n",
    "    scaled according to `xscale` and `yscale` respecitvely.\n",
    "    \n",
    "    Each corresponding element of `infos` and `regrets` should contain\n",
    "    a specification of an experiment with a particular ranking model\n",
    "    and corresponding regret, respectively.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: instance of matplotlib's axes\n",
    "        Target axes to plot the curves into.\n",
    "\n",
    "    infos: array\n",
    "        The description of the experiments.\n",
    "\n",
    "    regrets: array\n",
    "        Regrets of ranking models in corresponding experiments.\n",
    "    '''    \n",
    "    if len(set([info['query'] for info in infos])) != 1:\n",
    "        raise ValueError('Experiments to plot were not done with the same query.')\n",
    "        \n",
    "    if len(set([info['cutoff'] for info in infos])) != 1:\n",
    "        raise ValueError('Experiments to plot were not done with the same cutoff.')\n",
    "    \n",
    "    if len(set([info['click_model'].getName() for info in infos])) != 1:\n",
    "        raise ValueError('Experiments to plot were not done with the same click model.')\n",
    "    \n",
    "    qid = infos[0]['query']\n",
    "    cutoff = infos[0]['cutoff']\n",
    "    click_model_name = infos[0]['click_model'].getName()\n",
    "    \n",
    "    xscale = 'linear' if xscale is None else xscale\n",
    "    yscale = 'linear' if yscale is None else yscale\n",
    "    \n",
    "    ax.set_title('%s@%d - Q%s' % (click_model_name, cutoff, qid))\n",
    "    \n",
    "    ax.set_ylabel('Regret - (CTR@%d)' % cutoff)\n",
    "    ax.set_xlabel('Impressions')\n",
    "\n",
    "    ax.set_xscale(xscale)\n",
    "    ax.set_yscale(yscale)\n",
    "\n",
    "    ax.title.set_fontsize(10)\n",
    "\n",
    "    ax.xaxis.label.set_fontsize(10)\n",
    "    ax.yaxis.label.set_fontsize(10)\n",
    "\n",
    "    ax.xaxis.get_offset_text().set_fontsize(10)\n",
    "    ax.yaxis.get_offset_text().set_fontsize(10)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=10)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=10)\n",
    "    \n",
    "    # hsv = plt.get_cmap('hsv')\n",
    "    # colors = hsv(np.linspace(0, 1.0, len(infos)))\n",
    "\n",
    "    colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', '#f089cf']\n",
    "    \n",
    "    for info, regret, color in zip(infos, regrets, colors):\n",
    "        ranker_model_name = ranker2legend[info['ranking_model'].getName()]\n",
    "        \n",
    "        if not cumulative:\n",
    "            regret = regret.cumsum()\n",
    "\n",
    "        # Subsample regret if there is more than 10^5 iterations.\n",
    "        if regret.shape[0] > 100000:\n",
    "            indices = np.linspace(0, regret.shape[0] - 1, 100000).astype('int32')\n",
    "            regret = regret[indices]\n",
    "        else:\n",
    "            indices = np.arange(regret.shape[0], dtype='int32')\n",
    "            \n",
    "        # Pick a line style based on the feedback!\n",
    "        if info['ranking_model'].getName().endswith('[FC]'):\n",
    "            ls = '-.'\n",
    "        elif info['ranking_model'].getName().endswith('[LC]'):\n",
    "            ls = '--'\n",
    "        else:\n",
    "            ls = '-'\n",
    "\n",
    "        ax.plot(indices, regret, ls='-', color=color, lw=1, label=ranker_model_name)\n",
    "    \n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "\n",
    "def plot_average_regret_curves(ax, info, regret, xscale=None, yscale=None):\n",
    "    '''\n",
    "    Plots the regret curves for the experiment described in `info` with axis\n",
    "    scaled according to `xscale` and `yscale` respecitvely.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: instance of matplotlib's axes\n",
    "        Target axes to plot the curves into.\n",
    "\n",
    "    info:\n",
    "        The description of the experiment.\n",
    "\n",
    "    regret: array, shape = [4, n_impressions]\n",
    "        Matrix containing 4 row vectors storing mean, std, minima, and maxima\n",
    "        of cumulative regret, respectively.\n",
    "    '''\n",
    "    ranker_model_name = info['ranking_model'].getName()\n",
    "    click_model_name = info['click_model'].getName()\n",
    "    cutoff = info['cutoff']\n",
    "    qid = info['query']\n",
    "    \n",
    "    xscale = 'linear' if xscale is None else xscale\n",
    "    yscale = 'linear' if yscale is None else yscale\n",
    "    \n",
    "    # Subsample regret if there is more than 10^5 iterations.\n",
    "    if regret[0].shape[0] > 100000:\n",
    "        indices = np.linspace(0, regret[0].shape[0] - 1, 100000).astype('int32')\n",
    "        regret = regret[:, indices]\n",
    "    else:\n",
    "        indices = np.arange(regret[0].shape[0], dtype='int32')\n",
    "    \n",
    "    ax.set_title('%s - %s@%d - Q%s' % (ranker_model_name, click_model_name, cutoff, qid))\n",
    "\n",
    "    tmp1 = np.maximum(regret[0] - regret[1], regret[2])\n",
    "    tmp2 = np.minimum(regret[0] + regret[1], regret[3])\n",
    "    \n",
    "    ax.fill_between(indices, regret[2], tmp1, facecolor='red', edgecolor='none',\n",
    "                    alpha=0.2, interpolate=True)\n",
    "    \n",
    "    ax.fill_between(indices, tmp1, tmp2, facecolor='blue', edgecolor='none',\n",
    "                    alpha=0.2, interpolate=True)\n",
    "    \n",
    "    ax.fill_between(indices, tmp2, regret[3], facecolor='red', edgecolor='none',\n",
    "                    alpha=0.2, interpolate=True)\n",
    "    \n",
    "    ax.plot(indices, regret[0], 'k-')\n",
    "\n",
    "    ax.set_ylabel('Regret - (CTR@%d)' % cutoff)\n",
    "    ax.set_xlabel('Impressions')\n",
    "    \n",
    "    ax.set_ylim(bottom=0.0)\n",
    "\n",
    "    ax.set_xscale(xscale)\n",
    "    ax.set_yscale(yscale)\n",
    "\n",
    "    ax.title.set_fontsize(10)\n",
    "\n",
    "    ax.xaxis.label.set_fontsize(10)\n",
    "    ax.yaxis.label.set_fontsize(10)\n",
    "\n",
    "    ax.xaxis.get_offset_text().set_fontsize(10)\n",
    "    ax.yaxis.get_offset_text().set_fontsize(10)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=10)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=10)\n",
    "\n",
    "    \n",
    "def plot_multiple_average_regret_curves(ax, infos, regrets, xscale=None, yscale=None, xlim=None, reward=False):\n",
    "    '''\n",
    "    Plots the regret curves for the experiments described in `infos` with axis\n",
    "    scaled according to `xscale` and `yscale` respecitvely.\n",
    "    \n",
    "    Each corresponding element of `infos` and `regrets` should contain\n",
    "    a specification of repeated experiments with a particular ranking model\n",
    "    and their average regret, respectively.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: instance of matplotlib's axes\n",
    "        Target axes to plot the curves into.\n",
    "\n",
    "    info:\n",
    "        The description of the experiment.\n",
    "\n",
    "    regret: array, shape = [4, n_impressions]\n",
    "        Matrix containing 4 row vectors storing mean, std, minima, and maxima\n",
    "        of cumulative regret, respectively.\n",
    "    '''    \n",
    "    if len(set([info['query'] for info in infos])) != 1:\n",
    "        raise ValueError('Experiments to plot were not done with the same query.')\n",
    "        \n",
    "    if len(set([info['cutoff'] for info in infos])) != 1:\n",
    "        raise ValueError('Experiments to plot were not done with the same cutoff.')\n",
    "    \n",
    "    if len(set([info['click_model'].getName() for info in infos])) != 1:\n",
    "        raise ValueError('Experiments to plot were not done with the same click model.')\n",
    "    \n",
    "    qid = infos[0]['query']\n",
    "    cutoff = infos[0]['cutoff']\n",
    "    click_model_name = infos[0]['click_model'].getName()\n",
    "    \n",
    "    xscale = 'linear' if xscale is None else xscale\n",
    "    yscale = 'linear' if yscale is None else yscale\n",
    "    \n",
    "    ax.set_title('%s@%d - Q%s' % (click_model_name, cutoff, qid))\n",
    "    \n",
    "    if reward:\n",
    "        ax.set_ylabel('Reward - (CTR@%d)' % cutoff)\n",
    "    else:\n",
    "        ax.set_ylabel('Regret - (CTR@%d)' % cutoff)\n",
    "\n",
    "    ax.set_xlabel('Impressions')\n",
    "    \n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim)\n",
    "\n",
    "    ax.set_xscale(xscale)\n",
    "    ax.set_yscale(yscale)\n",
    "\n",
    "    ax.title.set_fontsize(10)\n",
    "\n",
    "    ax.xaxis.label.set_fontsize(10)\n",
    "    ax.yaxis.label.set_fontsize(10)\n",
    "\n",
    "    ax.xaxis.get_offset_text().set_fontsize(10)\n",
    "    ax.yaxis.get_offset_text().set_fontsize(10)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=10)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=10)\n",
    "    \n",
    "    # hsv = plt.get_cmap('hsv')\n",
    "    # colors = hsv(np.linspace(0, 1.0, len(infos)))\n",
    "    \n",
    "    colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', '#f089cf']\n",
    "\n",
    "    for info, regret, color in zip(infos, regrets, colors):\n",
    "        ranker_model_name = ranker2legend[info['ranking_model'].getName()]\n",
    "\n",
    "        # Subsample regret if there is more than 10^5 iterations.\n",
    "        if regret[0].shape[0] > 100000:\n",
    "            indices = np.linspace(0, regret[0].shape[0] - 1, 100000).astype('int32')\n",
    "            regret = regret[:, indices]\n",
    "        else:\n",
    "            indices = np.arange(regret[0].shape[0], dtype='int32')\n",
    "        \n",
    "        if regret.shape[0] > 1:\n",
    "            tmp1 = np.maximum(regret[0] - regret[1], regret[2])\n",
    "            tmp2 = np.minimum(regret[0] + regret[1], regret[3])\n",
    "\n",
    "            ax.fill_between(indices, regret[2], tmp1, facecolor=color, edgecolor='none',\n",
    "                            alpha=0.1, interpolate=True)\n",
    "\n",
    "            ax.fill_between(indices, tmp1, tmp2, facecolor=color, edgecolor='none',\n",
    "                            alpha=0.2, interpolate=True)\n",
    "\n",
    "            ax.fill_between(indices, tmp2, regret[3], facecolor=color, edgecolor='none',\n",
    "                            alpha=0.1, interpolate=True)\n",
    "        \n",
    "        ax.plot(indices, regret[0], ls='-', c=color, label=ranker_model_name)\n",
    "    \n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "\n",
    "def plot_multiple_average_reward_curves(ax, infos, regrets, xscale=None, yscale=None, xlim=None):\n",
    "    '''\n",
    "    Plots the reward curves for the experiments described in `infos` with axis\n",
    "    scaled according to `xscale` and `yscale` respecitvely.\n",
    "    \n",
    "    Each corresponding element of `infos` and `rewards` should contain\n",
    "    a specification of repeated experiments with a particular ranking model\n",
    "    and their average regret, respectively.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: instance of matplotlib's axes\n",
    "        Target axes to plot the curves into.\n",
    "\n",
    "    info:\n",
    "        The description of the experiment.\n",
    "\n",
    "    regrets: array, shape = [4, n_impressions] or [n_impressions,]\n",
    "        Matrix containing either 4 row vectors storing mean, std, minima, and maxima\n",
    "        of cumulative reward, respectively, or just mean of cumulative reward.\n",
    "    '''    \n",
    "    if len(set([info['cutoff'] for info in infos])) != 1:\n",
    "        raise ValueError('Experiments to plot were not done with the same cutoff.')\n",
    "    \n",
    "    if len(set([info['click_model'].getName() for info in infos])) != 1:\n",
    "        raise ValueError('Experiments to plot were not done with the same click model.')\n",
    "\n",
    "    cutoff = infos[0]['cutoff']\n",
    "    click_model_name = infos[0]['click_model'].getName()\n",
    "    \n",
    "    xscale = 'linear' if xscale is None else xscale\n",
    "    yscale = 'linear' if yscale is None else yscale\n",
    "    \n",
    "    ax.set_title('%s@%d' % (click_model_name, cutoff))\n",
    "\n",
    "    ax.set_ylabel('Reward - (CTR@%d)' % cutoff)\n",
    "    ax.set_xlabel('Impressions')\n",
    "    \n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim)\n",
    "\n",
    "    ax.set_xscale(xscale)\n",
    "    ax.set_yscale(yscale)\n",
    "\n",
    "    ax.title.set_fontsize(10)\n",
    "\n",
    "    ax.xaxis.label.set_fontsize(10)\n",
    "    ax.yaxis.label.set_fontsize(10)\n",
    "\n",
    "    ax.xaxis.get_offset_text().set_fontsize(10)\n",
    "    ax.yaxis.get_offset_text().set_fontsize(10)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=10)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=10)\n",
    "    \n",
    "    # hsv = plt.get_cmap('hsv')\n",
    "    # colors = hsv(np.linspace(0, 1.0, len(infos)))\n",
    "    \n",
    "    colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', '#f089cf']\n",
    "\n",
    "    for info, regret, color in zip(infos, regrets, colors):\n",
    "        ranker_model_name = ranker2legend[info['ranking_model'].getName()]\n",
    "\n",
    "        # Subsample regret if there is more than 10^5 iterations.\n",
    "        if regret[0].shape[0] > 100000:\n",
    "            indices = np.linspace(0, regret[0].shape[0] - 1, 100000).astype('int32')\n",
    "            regret = regret[:, indices]\n",
    "        else:\n",
    "            indices = np.arange(regret[0].shape[0], dtype='int32')\n",
    "        \n",
    "        if regret.shape[0] > 1:\n",
    "            tmp1 = np.maximum(regret[0] - regret[1], regret[2])\n",
    "            tmp2 = np.minimum(regret[0] + regret[1], regret[3])\n",
    "\n",
    "            ax.fill_between(indices, regret[2], tmp1, facecolor=color, edgecolor='none',\n",
    "                            alpha=0.1, interpolate=True)\n",
    "\n",
    "            ax.fill_between(indices, tmp1, tmp2, facecolor=color, edgecolor='none',\n",
    "                            alpha=0.2, interpolate=True)\n",
    "\n",
    "            ax.fill_between(indices, tmp2, regret[3], facecolor=color, edgecolor='none',\n",
    "                            alpha=0.1, interpolate=True)\n",
    "        \n",
    "        ax.plot(indices, regret[0], ls='-', c=color, label=ranker_model_name)\n",
    "    \n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "\n",
    "def plot_optimal_impressions(ax, info, n_impressions):\n",
    "    ranker_model_name = info['ranking_model'].getName()\n",
    "    click_model_name = info['click_model'].getName()\n",
    "    cutoff = info['cutoff']\n",
    "    qid = info['query']\n",
    "    \n",
    "    # Subsample n_impressions if there is more than 10^5 iterations.\n",
    "    if n_impressions.shape[0] > 100000:\n",
    "        indices = np.linspace(0, n_impressions.shape[0] - 1, 100000).astype('int32')\n",
    "        n_impressions = n_impressions[indices]\n",
    "    else:\n",
    "        indices = np.arange(n_impressions.shape[0], dtype='int32')\n",
    "\n",
    "    ax.set_title('%s - %s@%d - Q%s' % (ranker_model_name, click_model_name, cutoff, qid))\n",
    "\n",
    "    ax.plot(indices, n_impressions, 'k-')\n",
    "\n",
    "    ax.set_ylabel('Impressions')\n",
    "    ax.set_xlabel('Iteration')\n",
    "\n",
    "    # ax.set_ylim([0, 1])\n",
    "\n",
    "    ax.title.set_fontsize(10)\n",
    "\n",
    "    ax.xaxis.label.set_fontsize(10)\n",
    "    ax.yaxis.label.set_fontsize(10)\n",
    "\n",
    "    ax.xaxis.get_offset_text().set_fontsize(10)\n",
    "    ax.yaxis.get_offset_text().set_fontsize(10)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=10)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative Regret Curves for Ranking and Click Model Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cPickle as pickle\n",
    "from ipywidgets import Select, Dropdown, interactive, HBox\n",
    "from IPython.display import display\n",
    "\n",
    "# Change this to force the visualization of experiments under this directory.\n",
    "# DO NOT FORGET TO SET THESE TWO DIRECTORIES CONSISTENTLY!!!\n",
    "#EXPERIMENTS_DIRECTORY = 'experiments/custom/mr_ucb_vs_kl'\n",
    "EXPERIMENTS_DIRECTORY = 'experiments/mergerank_benchmark/RealMergeRankKLAlgorithm/run1'\n",
    "SAVE_PLOTS_DIRECTORY = 'figures'\n",
    "\n",
    "def get_regret_filepath(experiment_info_filepath):\n",
    "    return experiment_info_filepath.rstrip('experiment.nfo') + 'regret.npy'\n",
    "\n",
    "def show_regrets_widget(xscale='linear', yscale='linear', save_plots=False):\n",
    "    experiment_filepaths = []\n",
    "    experiment_specs = []\n",
    "\n",
    "    def show_regrets(ranking_model_name, click_model_name, xscale, yscale, save_plots=False):\n",
    "        spec_indices = [i for i, spec in enumerate(experiment_specs)\n",
    "                        if spec['click_model'].getName() == click_model_name and\n",
    "                        spec['ranking_model'].getName() == ranking_model_name and\n",
    "                        os.path.exists(get_regret_filepath(experiment_filepaths[i]))]\n",
    "        \n",
    "        specs, spec_indices = zip(*sorted([(experiment_specs[i], i) for i in spec_indices],\n",
    "                                          key=lambda info: info[0]['query']))\n",
    "        regrets = [np.load(get_regret_filepath(experiment_filepaths[i])) for i in spec_indices]\n",
    "\n",
    "        n_rows = (len(specs) + 1) / 2\n",
    "        n_cols = 2 if len(specs) > 1 else 1\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(9 * n_cols, 5 * n_rows))\n",
    "        \n",
    "        axes = axes.ravel() if len(specs) > 1 else [axes]\n",
    "        \n",
    "        for ax, info, regret in zip(axes, specs, regrets):\n",
    "            # regret = np.ceil(regret)\n",
    "            plot_regret_curves(ax, info, regret, xscale=xscale, yscale=yscale)\n",
    "        \n",
    "        if len(specs) > 1 and len(specs) % 2 == 1:\n",
    "            axes[-1].axis('off')\n",
    "\n",
    "        if save_plots:\n",
    "            filename = ranking_model_name + '_' + click_model_name + '_' + xscale + '_' + yscale + '.png'\n",
    "            fig.savefig(os.path.join(SAVE_PLOTS_DIRECTORY, filename), bbox_inches='tight')\n",
    "        else:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        plt.close(fig)\n",
    "\n",
    "    for root, dirs, files in os.walk(EXPERIMENTS_DIRECTORY, topdown=True, followlinks=True):\n",
    "        for fn in files:\n",
    "            if fn.endswith('.nfo'):\n",
    "                fp = os.path.join(root, fn)\n",
    "                experiment_filepaths.append(fp)\n",
    "                with open(fp) as ifile:\n",
    "                    experiment_specs.append(pickle.load(ifile))\n",
    "    \n",
    "    ranking_model_names = set([spec['ranking_model'].getName() for spec in experiment_specs])\n",
    "    click_model_names = set([spec['click_model'].getName() for spec in experiment_specs])\n",
    "    \n",
    "    if save_plots:\n",
    "        for rm, cm in set([(spec['ranking_model'].getName(), spec['click_model'].getName())\n",
    "                           for spec in experiment_specs]):\n",
    "            print 'Saving figures for', rm, cm, 'models...',\n",
    "            show_regrets(rm, cm, xscale, yscale, save_plots)\n",
    "            print 'done.'\n",
    "        return\n",
    "\n",
    "    rmdd = Select(options=list(ranking_model_names), description='Ranking Model:', width='150px')\n",
    "    cmdd = Select(options=list(click_model_names), description='Click Model:', width='75px')\n",
    "    xsdd = Select(options=['linear', 'sqrt', 'log'], description='X Scale:', width='100px', height='65px')\n",
    "    ysdd = Select(options=['linear', 'sqrt', 'log'], description='Y Scale:', width='100px', height='65px')\n",
    "    \n",
    "    controls = HBox([rmdd, cmdd, xsdd, ysdd])\n",
    "    backend = interactive(show_regrets, ranking_model_name=rmdd, click_model_name=cmdd,\n",
    "                          xscale=xsdd, yscale=ysdd)\n",
    "\n",
    "    controls.on_displayed(lambda _: show_regrets(rmdd.value, cmdd.value, xsdd.value, ysdd.value))\n",
    "\n",
    "    display(controls)\n",
    "\n",
    "show_regrets_widget(xscale='linear', yscale='linear', save_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative Regret Curves for Multiple Ranking Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import groupby\n",
    "import cPickle as pickle\n",
    "from ipywidgets import Select, Dropdown, interactive, HBox\n",
    "from IPython.display import display\n",
    "\n",
    "# Change this to force the visualization of experiments under this directory.\n",
    "# DO NOT FORGET TO SET THESE TWO DIRECTORIES CONSISTENTLY!!!\n",
    "EXPERIMENTS_DIRECTORY = 'experiments/60Q/yandex/MergeRankComparison'\n",
    "SAVE_PLOTS_DIRECTORY = 'figures/yandex'\n",
    "\n",
    "if not os.path.isdir(SAVE_PLOTS_DIRECTORY):\n",
    "    os.makedirs(SAVE_PLOTS_DIRECTORY)\n",
    "\n",
    "def get_regret_filepath(experiment_info_filepath):\n",
    "    return experiment_info_filepath.rstrip('experiment.nfo') + 'regret.npy'\n",
    "\n",
    "def show_regrets_widget(click_models=None, xscale='log', yscale='log', save_plots=False, filename_prefix=''):\n",
    "    experiment_filepaths = []\n",
    "    experiment_specs = []\n",
    "\n",
    "    def show_regrets(click_model_name, xscale, yscale, save_plots=False, filename_prefix=''):\n",
    "        spec_indices = [i for i, spec in enumerate(experiment_specs)\n",
    "                        if spec['click_model'].getName() == click_model_name and\n",
    "                        os.path.exists(get_regret_filepath(experiment_filepaths[i]))]\n",
    "        \n",
    "        q_specs = {}\n",
    "        q_regrets = {}\n",
    "        \n",
    "        for q, g1 in groupby(sorted([(experiment_specs[i], i) for i in spec_indices],\n",
    "                                         key=lambda info: info[0]['query']),\n",
    "                             key=lambda info: info[0]['query']):\n",
    "            q_rm_specs, q_rm_spec_indices = zip(*g1)\n",
    "\n",
    "            # The only thing that can go wrong in this place is that the number \n",
    "            # of impressions or the cut-off were different in different experiments\n",
    "            # on the same query and click model across different ranking models!\n",
    "            n_impressions_check = set([q_rm_spec['n_impressions'] for q_rm_spec in q_rm_specs])\n",
    "            cutoff_check = set([q_rm_spec['cutoff'] for q_rm_spec in q_rm_specs])\n",
    "\n",
    "            if len(n_impressions_check) != 1:\n",
    "                raise ValueError('Detected different number of impressions in experiments '\n",
    "                                 ' with query %s, ranking model %s, and click model %s: %r'\n",
    "                                % (q, q_rm_specs[0]['ranking_model'],\n",
    "                                   click_model_name, list(n_impressions_check)))\n",
    "\n",
    "            if len(cutoff_check) != 1:\n",
    "                raise ValueError('Detected different settings of cutoff in experiments '\n",
    "                                 ' with query %s, ranking model %s, and click model %s: %r'\n",
    "                                % (q, q_rm_specs[0]['ranking_model'],\n",
    "                                   click_model_name, list(cutoff_check)))\n",
    "\n",
    "            if q not in q_specs:\n",
    "                q_specs[q] = []\n",
    "                q_regrets[q] = []\n",
    "            \n",
    "            q_specs[q].extend(q_rm_specs)\n",
    "            q_regrets[q].extend([np.load(get_regret_filepath(experiment_filepaths[i]))\n",
    "                                 for i in q_rm_spec_indices])\n",
    "\n",
    "        n_rows = (len(q_specs) + 1) / 2\n",
    "        n_cols = 2 if len(q_specs) > 1 else 1\n",
    "\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(9 * n_cols, 5 * n_rows))\n",
    "\n",
    "        axes = axes.ravel() if len(q_specs) > 1 else [axes]\n",
    "        \n",
    "        qs = sorted(q_specs.keys())\n",
    "\n",
    "        for ax, infos, regrets in zip(axes, [q_specs[q] for q in qs], [q_regrets[q] for q in qs]):\n",
    "            plot_multiple_regret_curves(ax, infos, regrets, xscale=xscale,\n",
    "                                        yscale=yscale)\n",
    "\n",
    "        if len(q_specs) > 1 and len(q_specs) % 2 == 1:\n",
    "            axes[-1].axis('off')\n",
    "\n",
    "        if save_plots:\n",
    "            filename = filename_prefix + click_model_name + '_' + xscale + '_' + yscale + '.png'\n",
    "            fig.savefig(os.path.join(SAVE_PLOTS_DIRECTORY, filename), bbox_inches='tight')\n",
    "        else:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        plt.close(fig)\n",
    "\n",
    "    for root, dirs, files in os.walk(EXPERIMENTS_DIRECTORY, topdown=True, followlinks=True):\n",
    "        for fn in files:\n",
    "            if fn.endswith('.nfo'):\n",
    "                fp = os.path.join(root, fn)\n",
    "                experiment_filepaths.append(fp)\n",
    "                with open(fp) as ifile:\n",
    "                    experiment_specs.append(pickle.load(ifile))\n",
    "\n",
    "    if save_plots:\n",
    "        if click_models is None:\n",
    "            click_models = set([spec['click_model'].getName() for spec in experiment_specs])\n",
    "            \n",
    "        for cm in click_models:\n",
    "            print 'Saving figures for', cm, 'model...',\n",
    "            show_regrets(cm, xscale, yscale, save_plots, filename_prefix)\n",
    "            print 'done.'\n",
    "        return\n",
    "\n",
    "    click_model_names = set([spec['click_model'].getName() for spec in experiment_specs])\n",
    "\n",
    "    cmdd = Select(options=list(click_model_names), description='Click Model:', width='75px')\n",
    "    xsdd = Select(options=['linear', 'sqrt', 'log'], description='X Scale:', width='100px', height='65px')\n",
    "    ysdd = Select(options=['linear', 'sqrt', 'log'], description='Y Scale:', width='100px', height='65px')\n",
    "\n",
    "    controls = HBox([cmdd, xsdd, ysdd])\n",
    "    backend = interactive(show_regrets, click_model_name=cmdd, xscale=xsdd, yscale=ysdd)\n",
    "\n",
    "    controls.on_displayed(lambda _: show_regrets(cmdd.value, xsdd.value, ysdd.value))\n",
    "\n",
    "    display(controls)\n",
    "\n",
    "show_regrets_widget(xscale='linear', yscale='linear', save_plots=True, filename_prefix='MergeRankWithAndWitoutReset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative Regret Curves for Query and Click Model Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cPickle as pickle\n",
    "from ipywidgets import Select, Dropdown, interactive, HBox\n",
    "from IPython.display import display\n",
    "\n",
    "# Change this to force the visualization of experiments under this directory.\n",
    "EXPERIMENTS_DIRECTORY = 'experiments'\n",
    "\n",
    "def get_regret_filepath(experiment_info_filepath):\n",
    "    return experiment_info_filepath.rstrip('experiment.nfo') + 'regret.npy'\n",
    "\n",
    "def show_regrets_widget():\n",
    "    experiment_filepaths = []\n",
    "    experiment_specs = []\n",
    "    \n",
    "    def show_regrets(query, click_model_name, xscale, yscale):\n",
    "        spec_indices = [i for i, spec in enumerate(experiment_specs)\n",
    "                        if spec['click_model'].getName() == click_model_name and\n",
    "                        spec['query'] == query and\n",
    "                        os.path.exists(get_regret_filepath(experiment_filepaths[i]))]\n",
    "        specs, spec_indices = zip(*sorted([(experiment_specs[i], i) for i in spec_indices],\n",
    "                                          key=lambda info: info[0]['query']))\n",
    "        regrets = [np.load(get_regret_filepath(experiment_filepaths[i])) for i in spec_indices]\n",
    "\n",
    "        n_rows = (len(specs) + 1) / 2\n",
    "        n_cols = 2 if len(specs) > 1 else 1\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 2.5 * n_rows))\n",
    "        \n",
    "        axes = axes.ravel() if len(specs) > 1 else [axes]\n",
    "        \n",
    "        for ax, info, regret in zip(axes, specs, regrets):\n",
    "            plot_regret_curves(ax, info, regret, xscale=xscale, yscale=yscale)\n",
    "        \n",
    "        if len(specs) > 1 and len(specs) % 2 == 1:\n",
    "            axes[-1].axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()   \n",
    "\n",
    "    for root, dirs, files in os.walk(EXPERIMENTS_DIRECTORY, topdown=True):\n",
    "        for fn in files:\n",
    "            if fn.endswith('.nfo'):\n",
    "                fp = os.path.join(root, fn)\n",
    "                experiment_filepaths.append(fp)\n",
    "                with open(fp) as ifile:\n",
    "                    experiment_specs.append(pickle.load(ifile))\n",
    "    \n",
    "    queries = sorted(set([spec['query'] for spec in experiment_specs]))\n",
    "    click_model_names = set([spec['click_model'].getName() for spec in experiment_specs])\n",
    "\n",
    "    qdd = Select(options=list(queries), description='Queries:', width='150px', height='175px')\n",
    "    cmdd = Select(options=list(click_model_names), description='Click Model:', width='75px')\n",
    "    xsdd = Select(options=['linear', 'sqrt', 'log'], description='X Scale:', width='100px', height='65px')\n",
    "    ysdd = Select(options=['linear', 'sqrt', 'log'], description='Y Scale:', width='100px', height='65px')\n",
    "    \n",
    "    controls = HBox([qdd, cmdd, xsdd, ysdd])\n",
    "    print controls\n",
    "    backend = interactive(show_regrets, query=qdd, click_model_name=cmdd, xscale=xsdd, yscale=ysdd)\n",
    "\n",
    "    controls.on_displayed(lambda _: show_regrets(qdd.value, cmdd.value, xsdd.value, ysdd.value))\n",
    "\n",
    "    display(controls)\n",
    "\n",
    "show_regrets_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Cumulative Regret Curves for Single Ranking Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import groupby\n",
    "import cPickle as pickle\n",
    "from ipywidgets import Select, Dropdown, interactive, HBox\n",
    "from IPython.display import display\n",
    "\n",
    "# Change this to force the visualization of experiments under this directory.\n",
    "# DO NOT FORGET TO SET THESE TWO DIRECTORIES CONSISTENTLY!!!\n",
    "EXPERIMENTS_DIRECTORY = 'experiments/mergerank_benchmark/RealMergeRankKLAlgorithm'\n",
    "SAVE_PLOTS_DIRECTORY = 'figures/averages'\n",
    "\n",
    "if not os.path.isdir(SAVE_PLOTS_DIRECTORY):\n",
    "    os.makedirs(SAVE_PLOTS_DIRECTORY)\n",
    "\n",
    "def get_regret_filepath(experiment_info_filepath):\n",
    "    return experiment_info_filepath.rstrip('experiment.nfo') + 'regret.npy'\n",
    "\n",
    "def show_regrets_widget(xscale='linear', yscale='linear', save_plots=False):\n",
    "    experiment_filepaths = []\n",
    "    experiment_specs = []\n",
    "\n",
    "    def show_regrets(ranking_model_name, click_model_name, xscale, yscale, save_plots=False):\n",
    "        spec_indices = [i for i, spec in enumerate(experiment_specs)\n",
    "                        if spec['click_model'].getName() == click_model_name and\n",
    "                        spec['ranking_model'].getName() == ranking_model_name and\n",
    "                        os.path.exists(get_regret_filepath(experiment_filepaths[i]))]\n",
    "        \n",
    "        specs = []\n",
    "        regrets = []\n",
    "        \n",
    "        for q, g in groupby(sorted([(experiment_specs[i], i) for i in spec_indices],\n",
    "                                         key=lambda info: info[0]['query']),\n",
    "                                  key=lambda info: info[0]['query']):\n",
    "            q_specs, q_spec_indices = zip(*g)\n",
    "            \n",
    "            # The only thing that can go wrong in this place is that the number \n",
    "            # of impressions or the cut-off were different in different experiments\n",
    "            # on the same query, ranking model, and click model!\n",
    "            n_impressions_check = set([q_spec['n_impressions'] for q_spec in q_specs])\n",
    "            cutoff_check = set([q_spec['cutoff'] for q_spec in q_specs])\n",
    "            \n",
    "            if len(n_impressions_check) != 1:\n",
    "                raise ValueError('Detected different number of impressions in experiments '\n",
    "                                 ' with query %s, ranking model %s, and click model %s: %r'\n",
    "                                % (q, ranking_model_name, click_model_name, list(n_impressions_check)))\n",
    "            \n",
    "            if len(cutoff_check) != 1:\n",
    "                raise ValueError('Detected different settings of cutoff in experiments '\n",
    "                                 ' with query %s, ranking model %s, and click model %s: %r'\n",
    "                                % (q, ranking_model_name, click_model_name, list(cutoff_check)))\n",
    "            \n",
    "            regrets_collection = np.vstack([np.load(get_regret_filepath(experiment_filepaths[i]))\n",
    "                                  for i in q_spec_indices]).cumsum(axis=1)\n",
    "            \n",
    "            # All experiments specifications are equal (except for the ranking model,\n",
    "            # that is not used) so we use just the 1st one in the list.\n",
    "            specs.append(q_specs[0])\n",
    "            regrets.append(np.vstack([regrets_collection.mean(axis=0),\n",
    "                                      regrets_collection.std(axis=0),\n",
    "                                      regrets_collection.min(axis=0),\n",
    "                                      regrets_collection.max(axis=0)]))\n",
    "\n",
    "        n_rows = (len(specs) + 1) / 2\n",
    "        n_cols = 2 if len(specs) > 1 else 1\n",
    "\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(9 * n_cols, 5 * n_rows))\n",
    "\n",
    "        axes = axes.ravel() if len(specs) > 1 else [axes]\n",
    "\n",
    "        for ax, info, regret in zip(axes, specs, regrets):\n",
    "            plot_average_regret_curves(ax, info, regret, xscale=xscale, yscale=yscale)\n",
    "\n",
    "        if len(specs) > 1 and len(specs) % 2 == 1:\n",
    "            axes[-1].axis('off')\n",
    "\n",
    "        if save_plots:\n",
    "            filename = ranking_model_name + '_' + click_model_name + '_' + xscale + '_' + yscale + '.png'\n",
    "            fig.savefig(os.path.join(SAVE_PLOTS_DIRECTORY, filename), bbox_inches='tight')\n",
    "        else:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        plt.close(fig)\n",
    "\n",
    "    for root, dirs, files in os.walk(EXPERIMENTS_DIRECTORY, topdown=True):\n",
    "        for fn in files:\n",
    "            if fn.endswith('.nfo'):\n",
    "                fp = os.path.join(root, fn)\n",
    "                experiment_filepaths.append(fp)\n",
    "                with open(fp) as ifile:\n",
    "                    experiment_specs.append(pickle.load(ifile))\n",
    "    \n",
    "    if save_plots:\n",
    "        for rm, cm in set([(spec['ranking_model'].getName(), spec['click_model'].getName())\n",
    "                           for spec in experiment_specs]):\n",
    "            print 'Saving figures for', rm, cm, 'models...',\n",
    "            show_regrets(rm, cm, xscale, yscale, save_plots)\n",
    "            print 'done.'\n",
    "        return\n",
    "    \n",
    "    ranking_model_names = set([spec['ranking_model'].getName() for spec in experiment_specs])\n",
    "    click_model_names = set([spec['click_model'].getName() for spec in experiment_specs])\n",
    "\n",
    "    rmdd = Select(options=list(ranking_model_names), description='Ranking Model:', width='150px')\n",
    "    cmdd = Select(options=list(click_model_names), description='Click Model:', width='75px')\n",
    "    xsdd = Select(options=['linear', 'sqrt', 'log'], description='X Scale:', width='100px', height='65px')\n",
    "    ysdd = Select(options=['linear', 'sqrt', 'log'], description='Y Scale:', width='100px', height='65px')\n",
    "    \n",
    "    controls = HBox([rmdd, cmdd, xsdd, ysdd])\n",
    "    backend = interactive(show_regrets, ranking_model_name=rmdd, click_model_name=cmdd,\n",
    "                          xscale=xsdd, yscale=ysdd)\n",
    "\n",
    "    controls.on_displayed(lambda _: show_regrets(rmdd.value, cmdd.value, xsdd.value, ysdd.value))\n",
    "\n",
    "    display(controls)\n",
    "\n",
    "show_regrets_widget(save_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Cumulative Regret Curves for Multiple Ranking Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import groupby\n",
    "import cPickle as pickle\n",
    "from ipywidgets import Select, Dropdown, interactive, HBox\n",
    "from IPython.display import display\n",
    "\n",
    "# Change this to force the visualization of experiments under this directory.\n",
    "# DO NOT FORGET TO SET THESE TWO DIRECTORIES CONSISTENTLY!!!\n",
    "EXPERIMENTS_DIRECTORY = 'experiments/mergerank_benchmark/algorithms'\n",
    "SAVE_PLOTS_DIRECTORY = 'figures/averages'\n",
    "\n",
    "if not os.path.isdir(SAVE_PLOTS_DIRECTORY):\n",
    "    os.makedirs(SAVE_PLOTS_DIRECTORY)\n",
    "\n",
    "def get_regret_filepath(experiment_info_filepath):\n",
    "    return experiment_info_filepath.rstrip('experiment.nfo') + 'regret.npy'\n",
    "\n",
    "def show_regrets_widget(click_models=None, xscale='log', yscale='linear', xlim=None,\n",
    "                        save_plots=False, filename_prefix='', average_only=False, reward=False):\n",
    "    experiment_filepaths = []\n",
    "    experiment_specs = []\n",
    "\n",
    "    def show_regrets(click_model_name, xscale, yscale, save_plots=False, filename_prefix=''):\n",
    "        spec_indices = [i for i, spec in enumerate(experiment_specs)\n",
    "                        if spec['click_model'].getName() == click_model_name and\n",
    "                        os.path.exists(get_regret_filepath(experiment_filepaths[i]))]\n",
    "        \n",
    "        q_specs = {}\n",
    "        q_regrets = {}\n",
    "        \n",
    "        for q, g1 in groupby(sorted([(experiment_specs[i], i) for i in spec_indices],\n",
    "                                         key=lambda info: info[0]['query']),\n",
    "                             key=lambda info: info[0]['query']):\n",
    "        \n",
    "            _, q_spec_indices = zip(*g1)\n",
    "\n",
    "            for q_rm, g2 in groupby(sorted([(experiment_specs[i], i) for i in q_spec_indices],\n",
    "                                           key=lambda info: info[0]['ranking_model'].getName()),\n",
    "                                    key=lambda info: info[0]['ranking_model'].getName()):\n",
    "            \n",
    "                q_rm_specs, q_rm_spec_indices = zip(*g2)\n",
    "            \n",
    "                # The only thing that can go wrong in this place is that the number \n",
    "                # of impressions or the cut-off were different in different experiments\n",
    "                # on the same query, ranking model, and click model!\n",
    "                n_impressions_check = set([q_rm_spec['n_impressions'] for q_rm_spec in q_rm_specs])\n",
    "                cutoff_check = set([q_rm_spec['cutoff'] for q_rm_spec in q_rm_specs])\n",
    "\n",
    "                if len(n_impressions_check) != 1:\n",
    "                    raise ValueError('Detected different number of impressions in experiments '\n",
    "                                     ' with query %s, ranking model %s, and click model %s: %r'\n",
    "                                    % (q, q_rm_specs[0]['ranking_model'],\n",
    "                                       click_model_name, list(n_impressions_check)))\n",
    "\n",
    "                if len(cutoff_check) != 1:\n",
    "                    raise ValueError('Detected different settings of cutoff in experiments '\n",
    "                                     ' with query %s, ranking model %s, and click model %s: %r'\n",
    "                                    % (q, q_rm_specs[0]['ranking_model'],\n",
    "                                       click_model_name, list(cutoff_check)))\n",
    "                \n",
    "                click_model = q_rm_specs[0]['click_model']\n",
    "                n_impressions = q_rm_specs[0]['n_impressions']\n",
    "                n_documents = q_rm_specs[0]['n_documents']\n",
    "                cutoff = q_rm_specs[0]['cutoff']\n",
    "                \n",
    "                # Get the ideal top-`cutoff` ranking for the query and click model ...\n",
    "                ideal_ranking = click_model.get_ideal_ranking(cutoff=cutoff)\n",
    "\n",
    "                # ... and compute its clickthrough rate.\n",
    "                ideal_ctr = click_model.get_clickthrough_rate(ideal_ranking,\n",
    "                                                              np.arange(n_documents, dtype='int32'),\n",
    "                                                              cutoff=cutoff)\n",
    "\n",
    "                regrets_collection = np.vstack([np.load(get_regret_filepath(experiment_filepaths[i]))\n",
    "                                      for i in q_rm_spec_indices]).cumsum(axis=1)\n",
    "\n",
    "                if q not in q_specs:\n",
    "                    q_specs[q] = []\n",
    "                    q_regrets[q] = []\n",
    "                    \n",
    "                # Contents of all experiment specifications are equal (except for the ranking model,\n",
    "                # so we use just the 1st one in the list.\n",
    "                q_specs[q].append(q_rm_specs[0])\n",
    "                \n",
    "                if reward:\n",
    "                    regrets_collection /= np.arange(1, 1 + n_impressions, dtype='f4')[None, :]\n",
    "                    regrets_collection = ideal_ctr - regrets_collection\n",
    "                    \n",
    "                    if average_only:\n",
    "                        q_regrets[q].append(regrets_collection.mean(axis=0)[None, :])\n",
    "                    else:\n",
    "                        q_regrets[q].append(np.vstack([regrets_collection.mean(axis=0),\n",
    "                                                       regrets_collection.std(axis=0),\n",
    "                                                       regrets_collection.min(axis=0),\n",
    "                                                       regrets_collection.max(axis=0)]))\n",
    "                else:\n",
    "                    if average_only:\n",
    "                        q_regrets[q].append(regrets_collection.mean(axis=0)[None, :])\n",
    "                    else:\n",
    "                        q_regrets[q].append(np.vstack([regrets_collection.mean(axis=0),\n",
    "                                                       regrets_collection.std(axis=0),\n",
    "                                                       regrets_collection.min(axis=0),\n",
    "                                                       regrets_collection.max(axis=0)]))\n",
    "                del regrets_collection\n",
    "                \n",
    "        n_rows = (len(q_specs) + 1) / 2\n",
    "        n_cols = 2 if len(q_specs) > 1 else 1\n",
    "\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(9 * n_cols, 5 * n_rows))\n",
    "\n",
    "        axes = axes.ravel() if len(q_specs) > 1 else [axes]\n",
    "\n",
    "        qs = sorted(q_specs.keys())\n",
    "\n",
    "        for ax, infos, regrets in zip(axes, [q_specs[q] for q in qs], [q_regrets[q] for q in qs]):\n",
    "            plot_multiple_average_regret_curves(ax, infos, regrets, xscale=xscale,\n",
    "                                                yscale=yscale, xlim=xlim, reward=reward)\n",
    "\n",
    "        if len(q_specs) > 1 and len(q_specs) % 2 == 1:\n",
    "            axes[-1].axis('off')\n",
    "\n",
    "        if save_plots:\n",
    "            filename = filename_prefix + click_model_name + '_AVG_REG_' + xscale + '_' + yscale + '.png'\n",
    "            fig.savefig(os.path.join(SAVE_PLOTS_DIRECTORY, filename), bbox_inches='tight')\n",
    "        else:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        plt.close(fig)\n",
    "\n",
    "    for root, dirs, files in os.walk(EXPERIMENTS_DIRECTORY, topdown=True, followlinks=True):\n",
    "        for fn in files:\n",
    "            if fn.endswith('.nfo'):\n",
    "                fp = os.path.join(root, fn)\n",
    "                experiment_filepaths.append(fp)\n",
    "                with open(fp) as ifile:\n",
    "                    experiment_specs.append(pickle.load(ifile))\n",
    "\n",
    "    if save_plots:\n",
    "        if click_models is None:\n",
    "            click_models = set([spec['click_model'].getName() for spec in experiment_specs])\n",
    "            \n",
    "        for cm in click_models:\n",
    "            print 'Saving figures for', cm, 'model...',\n",
    "            show_regrets(cm, xscale, yscale, save_plots, filename_prefix)\n",
    "            print 'done.'\n",
    "        return\n",
    "\n",
    "    click_model_names = set([spec['click_model'].getName() for spec in experiment_specs])\n",
    "\n",
    "    cmdd = Select(options=list(click_model_names), description='Click Model:', width='75px')\n",
    "    xsdd = Select(options=['linear', 'sqrt', 'log'], description='X Scale:', width='100px', height='65px')\n",
    "    ysdd = Select(options=['linear', 'sqrt', 'log'], description='Y Scale:', width='100px', height='65px')\n",
    "    \n",
    "    controls = HBox([cmdd, xsdd, ysdd])\n",
    "    backend = interactive(show_regrets, click_model_name=cmdd, xscale=xsdd, yscale=ysdd)\n",
    "\n",
    "    controls.on_displayed(lambda _: show_regrets(cmdd.value, xsdd.value, ysdd.value))\n",
    "\n",
    "    display(controls)\n",
    "\n",
    "# show_regrets_widget(xlim=[1e5, 1e7], save_plots=False)\n",
    "show_regrets_widget(xscale='linear', yscale='linear', average_only=True, reward=False,\n",
    "                    filename_prefix='MergeRankBenchmarkAveragesOnly_', save_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Cumulative Regret Curves over All Queries for Multiple Ranking Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import groupby\n",
    "import cPickle as pickle\n",
    "from ipywidgets import Select, Dropdown, interactive, HBox\n",
    "from IPython.display import display\n",
    "\n",
    "# Change this to force the visualization of experiments under this directory.\n",
    "# DO NOT FORGET TO SET THESE TWO DIRECTORIES CONSISTENTLY!!!\n",
    "EXPERIMENTS_DIRECTORY = 'experiments/mergerank_benchmark/algorithms'\n",
    "SAVE_PLOTS_DIRECTORY = 'figures/averages'\n",
    "\n",
    "if not os.path.isdir(SAVE_PLOTS_DIRECTORY):\n",
    "    os.makedirs(SAVE_PLOTS_DIRECTORY)\n",
    "\n",
    "def get_regret_filepath(experiment_info_filepath):\n",
    "    return experiment_info_filepath.rstrip('experiment.nfo') + 'regret.npy'\n",
    "\n",
    "def show_rewards_widget(click_models=None, xscale='log', yscale='linear', xlim=None,\n",
    "                        save_plots=False, filename_prefix='', average_only=False):\n",
    "    experiment_filepaths = []\n",
    "    experiment_specs = []\n",
    "\n",
    "    def show_rewards(click_model_name, xscale, yscale, save_plots=False, filename_prefix=''):\n",
    "        # Allowing averaging over cumulative rewards across multiple click models.\n",
    "        if not isinstance(click_model_name, list):\n",
    "            click_model_name = [click_model_name]\n",
    "\n",
    "        spec_indices = [i for i, spec in enumerate(experiment_specs)\n",
    "                        if spec['click_model'].getName() in click_model_name and\n",
    "                        os.path.exists(get_regret_filepath(experiment_filepaths[i]))]\n",
    "        \n",
    "        if len(spec_indices) == 0:\n",
    "            raise ValueError('no experiment was found for with %s click model(s)'\n",
    "                             % ', '.join(click_model_name))\n",
    "        \n",
    "        all_rm_specs = {}\n",
    "        all_rm_regrets = {}\n",
    "        \n",
    "        for rm, g in groupby(sorted([(experiment_specs[i], i) for i in spec_indices],\n",
    "                                    key=lambda info: info[0]['ranking_model'].getName()),\n",
    "                             key=lambda info: info[0]['ranking_model'].getName()):\n",
    "\n",
    "            rm_specs, rm_spec_indices = zip(*g)\n",
    "\n",
    "            # The only thing that can go wrong in this place is that the number \n",
    "            # of impressions or the cut-off were different in different experiments\n",
    "            # on the same query, ranking model, and click model!\n",
    "            n_impressions_check = set([rm_spec['n_impressions'] for rm_spec in rm_specs])\n",
    "            cutoff_check = set([rm_spec['cutoff'] for rm_spec in rm_specs])\n",
    "\n",
    "            if len(n_impressions_check) != 1:\n",
    "                raise ValueError('Detected different number of impressions in experiments '\n",
    "                                 ' with ranking model %s, and click model %s: %r'\n",
    "                                % (rm, click_model_name, list(n_impressions_check)))\n",
    "\n",
    "            if len(cutoff_check) != 1:\n",
    "                raise ValueError('Detected different settings of cutoff in experiments '\n",
    "                                 ' with ranking model %s, and click model %s: %r'\n",
    "                                % (rm, click_model_name, list(cutoff_check)))\n",
    "\n",
    "            click_models = [rm_spec['click_model'] for rm_spec in rm_specs]\n",
    "            n_impressions = rm_specs[0]['n_impressions']\n",
    "            n_documents = rm_specs[0]['n_documents']\n",
    "            cutoff = rm_specs[0]['cutoff']\n",
    "\n",
    "            ideal_ctrs = np.array([cm.get_clickthrough_rate(cm.get_ideal_ranking(cutoff=cutoff),\n",
    "                                                            np.arange(n_documents, dtype='int32'),\n",
    "                                                            cutoff=cutoff)\n",
    "                                   for cm in click_models])[:, None]\n",
    "\n",
    "            regrets_collection = np.vstack([np.load(get_regret_filepath(experiment_filepaths[i]))\n",
    "                                  for i in rm_spec_indices]).cumsum(axis=1)\n",
    "\n",
    "            # Contents of all experiment specifications are equal except for the query,\n",
    "            # so we use just the 1st one in the list.\n",
    "            all_rm_specs[rm] = rm_specs[0]\n",
    "\n",
    "            regrets_collection /= np.arange(1, 1 + n_impressions, dtype='f4')[None, :]\n",
    "            regrets_collection = ideal_ctrs - regrets_collection\n",
    "\n",
    "            if average_only:\n",
    "                all_rm_regrets[rm] = regrets_collection.mean(axis=0)[None, :]\n",
    "            else:\n",
    "                all_rm_regrets[rm] = np.vstack([regrets_collection.mean(axis=0),\n",
    "                                                regrets_collection.std(axis=0),\n",
    "                                                regrets_collection.min(axis=0),\n",
    "                                                regrets_collection.max(axis=0)])\n",
    "            del regrets_collection\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7 * np.sqrt(2), 7))\n",
    "\n",
    "        rms = sorted(all_rm_specs.keys())\n",
    "        \n",
    "        plot_multiple_average_reward_curves(ax, [all_rm_specs[rm] for rm in rms],\n",
    "                                            [all_rm_regrets[rm] for rm in rms],\n",
    "                                            xscale=xscale, yscale=yscale, xlim=xlim)\n",
    "\n",
    "        if save_plots:\n",
    "            filename = filename_prefix + '+'.join(click_model_name) + '_AVG_REWARD_' + xscale + '_' + yscale + '.png'\n",
    "            fig.savefig(os.path.join(SAVE_PLOTS_DIRECTORY, filename), bbox_inches='tight')\n",
    "        else:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        plt.close(fig)\n",
    "\n",
    "    for root, dirs, files in os.walk(EXPERIMENTS_DIRECTORY, topdown=True, followlinks=True):\n",
    "        for fn in files:\n",
    "            if fn.endswith('.nfo'):\n",
    "                fp = os.path.join(root, fn)\n",
    "                experiment_filepaths.append(fp)\n",
    "                with open(fp) as ifile:\n",
    "                    experiment_specs.append(pickle.load(ifile))\n",
    "\n",
    "    if save_plots:\n",
    "        if click_models is None:\n",
    "            click_models = set([spec['click_model'].getName() for spec in experiment_specs])\n",
    "            \n",
    "        for cm in click_models:\n",
    "            print 'Saving figures for', cm, 'model...',\n",
    "            show_rewards(cm, xscale, yscale, save_plots, filename_prefix)\n",
    "            print 'done.'\n",
    "        return\n",
    "\n",
    "    click_model_names = set([spec['click_model'].getName() for spec in experiment_specs])\n",
    "\n",
    "    cmdd = Select(options=list(click_model_names), description='Click Model:', width='75px')\n",
    "    xsdd = Select(options=['linear', 'sqrt', 'log'], description='X Scale:', width='100px', height='65px')\n",
    "    ysdd = Select(options=['linear', 'sqrt', 'log'], description='Y Scale:', width='100px', height='65px')\n",
    "\n",
    "    # Set the initially selected axis scale.\n",
    "    xsdd.value = xscale\n",
    "    ysdd.value = yscale\n",
    "    \n",
    "    controls = HBox([cmdd, xsdd, ysdd])\n",
    "    backend = interactive(show_rewards, click_model_name=cmdd, xscale=xsdd, yscale=ysdd)\n",
    "\n",
    "    controls.on_displayed(lambda _: show_rewards(cmdd.value, xsdd.value, ysdd.value))\n",
    "\n",
    "    display(controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_rewards_widget(xscale='log', yscale='linear', average_only=True,\n",
    "                    filename_prefix='MergeRankBenchmarkRewardAveragesOnly_', save_plots=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These will save the average cumulative reward for each ranking algorithm across all\n",
    "# the experiments stored under directory `EXPERIMENTS_DIRECTORY` in the cell\n",
    "# above. The click models over which the reward is averaged is determined by the first\n",
    "# parameter - `click_models`.\n",
    "\n",
    "# If you want to plot 1 sigma, min/max bounds, set `average_only` to False.\n",
    "\n",
    "show_rewards_widget(click_models=['PBM'], xscale='log', yscale='linear', average_only=True,\n",
    "                    filename_prefix='MergeRankBenchmarkRewardAveragesOnly_', save_plots=True)\n",
    "\n",
    "show_rewards_widget(click_models=['CM'], xscale='log', yscale='linear', average_only=True,\n",
    "                    filename_prefix='MergeRankBenchmarkRewardAveragesOnly_', save_plots=True)\n",
    "\n",
    "show_rewards_widget(click_models=[['PBM', 'CM']] , xscale='log', yscale='linear', average_only=True,\n",
    "                    filename_prefix='MergeRankBenchmarkRewardAveragesOnly_', save_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward in Time Relative Number of Impressions with Optimal Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cPickle as pickle\n",
    "from ipywidgets import Select, Dropdown, interactive, HBox\n",
    "from IPython.display import display\n",
    "\n",
    "# Change this to force the visualization of experiments under this directory.\n",
    "EXPERIMENTS_DIRECTORY = 'experiments'\n",
    "\n",
    "def get_rankings_filepath(experiment_info_filepath):\n",
    "    return experiment_info_filepath.rstrip('experiment.nfo') + 'rankings.npy'\n",
    "\n",
    "def compute_n_impressions(rankings, optimal_ranking):\n",
    "    C = 1.0 * (rankings == optimal_ranking).all(axis=1)[::-1].cumsum()\n",
    "    C /= np.arange(1, 1 + rankings.shape[0])\n",
    "    return C[::-1]\n",
    "\n",
    "def show_regrets_widget(save_plots=False):\n",
    "    experiment_filepaths = []\n",
    "    experiment_specs = []\n",
    "\n",
    "    def show_regrets(ranking_model_name, click_model_name, save_plots=False):\n",
    "        spec_indices = [i for i, spec in enumerate(experiment_specs)\n",
    "                        if spec['click_model'].getName() == click_model_name and\n",
    "                        spec['ranking_model'].getName() == ranking_model_name and\n",
    "                        os.path.exists(get_rankings_filepath(experiment_filepaths[i]))]\n",
    "        \n",
    "        specs, spec_indices = zip(*sorted([(experiment_specs[i], i) for i in spec_indices],\n",
    "                                          key=lambda info: info[0]['query']))\n",
    "        specs_n_impressions = []\n",
    "        \n",
    "        for i in spec_indices:\n",
    "            rankings = np.load(get_rankings_filepath(experiment_filepaths[i]))\n",
    "            cutoff = experiment_specs[i]['cutoff']\n",
    "            optimal_ranking = experiment_specs[i]['click_model'].get_ideal_ranking(cutoff, satisfied=False)\n",
    "            specs_n_impressions.append(compute_n_impressions(rankings[:, :cutoff], optimal_ranking))\n",
    "                           \n",
    "        n_rows = (len(specs) + 1) / 2\n",
    "        n_cols = 2 if len(specs) > 1 else 1\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 2.5 * n_rows))\n",
    "        \n",
    "        axes = axes.ravel() if len(specs) > 1 else [axes]\n",
    "        \n",
    "        for ax, info, n_impressions in zip(axes, specs, specs_n_impressions):\n",
    "            plot_optimal_impressions(ax, info, n_impressions)\n",
    "        \n",
    "        if len(specs) > 1 and len(specs) % 2 == 1:\n",
    "            axes[-1].axis('off')\n",
    "\n",
    "        if save_plots:\n",
    "            fig.savefig('Figs/' + ranking_model_name + '_' + click_model_name + '_OptImpCnt.pdf',\n",
    "                        bbox_inches='tight')\n",
    "        else:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    for root, dirs, files in os.walk(EXPERIMENTS_DIRECTORY, topdown=True):\n",
    "        for fn in files:\n",
    "            if fn.endswith('.nfo'):\n",
    "                fp = os.path.join(root, fn)\n",
    "                experiment_filepaths.append(fp)\n",
    "                with open(fp) as ifile:\n",
    "                    experiment_specs.append(pickle.load(ifile))\n",
    "    \n",
    "    ranking_model_names = set([spec['ranking_model'].getName() for spec in experiment_specs])\n",
    "    click_model_names = set([spec['click_model'].getName() for spec in experiment_specs])\n",
    "    \n",
    "    if save_plots:\n",
    "        for gl_ranking_model_name in ranking_model_names:\n",
    "            for gl_click_model_name in click_model_names:\n",
    "                print (gl_ranking_model_name, gl_click_model_name, save_plots)\n",
    "                show_regrets(gl_ranking_model_name, gl_click_model_name, save_plots)\n",
    "        return\n",
    "\n",
    "    rmdd = Select(options=list(ranking_model_names), description='Ranking Model:', width='150px')\n",
    "    cmdd = Select(options=list(click_model_names), description='Click Model:', width='75px')\n",
    "    \n",
    "    controls = HBox([rmdd, cmdd])\n",
    "    backend = interactive(show_regrets, ranking_model_name=rmdd, click_model_name=cmdd)\n",
    "\n",
    "    controls.on_displayed(lambda _: show_regrets(rmdd.value, cmdd.value))\n",
    "\n",
    "    display(controls)\n",
    "\n",
    "show_regrets_widget(save_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Click Model Attractiveness Probabilities of Documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, Dropdown, FloatText, VBox, HBox, fixed\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "SAVE_PLOTS_DIRECTORY = 'figures'\n",
    "\n",
    "with open('data/60Q/model_query_collection.pkl') as ifile:\n",
    "    MQD = pickle.load(ifile)\n",
    "\n",
    "def show_all_queries_relevances(click_model_name, save_plots=fixed(False)):\n",
    "    fig, axes = plt.subplots((len(MQD.values()[0]) + 1) / 2, 2, figsize=(12, len(MQD.values()[0])),\n",
    "                             sharex=True, sharey=True)\n",
    "\n",
    "    for i, query in enumerate(sorted(MQD[click_model_name].keys())):\n",
    "        ax = axes[i / 2, i % 2]\n",
    "\n",
    "        relevances = sorted(MQD[click_model_name][query]['relevances'], reverse=True)\n",
    "\n",
    "        ax.bar(np.arange(len(relevances)), relevances, 0.75,\n",
    "               alpha=0.7, color='green', align='center')\n",
    "\n",
    "        ax.set_title('Query: %s' % query)\n",
    "        ax.grid(axis='y', which='major')\n",
    "        \n",
    "        b = 'on' if i / 2 == 4 else 'off'\n",
    "        lb = 'on' if i / 2 == 4 else 'off'\n",
    "        l = 'on' if i % 2 == 0 else 'off'\n",
    "        ll = 'on' if i % 2 == 0 else 'off'\n",
    "                \n",
    "        ax.tick_params(axis='both', which='major', left=l, direction='out',\n",
    "                       top='off', right='off', bottom=b, labelleft=ll,\n",
    "                       labeltop='off', labelright='off', labelbottom=lb)\n",
    "            \n",
    "    ax.xaxis.set_major_locator(MultipleLocator(1.0))\n",
    "    ax.set_xlim([-0.5, len(relevances) - 0.5])\n",
    "    plt.tight_layout()\n",
    "    if save_plots:\n",
    "        filename = click_model_name + '_relevances.png'\n",
    "        fig.savefig(os.path.join(SAVE_PLOTS_DIRECTORY, filename), bbox_inches='tight')\n",
    "    else:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# To save the plots into `SAVE_PLOTS_DIRECTORY`.\n",
    "# for cm in MQD:\n",
    "#     show_all_queries_relevances(cm, save_plots=True)\n",
    "    \n",
    "_ = interact(show_all_queries_relevances, click_model_name=Dropdown(options=MQD.keys(), description='Click Model:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PBM Attractivenesses and Examination Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_PBM_queries_relevances(collection, cutoff=5, output_filename=None):\n",
    "    with open(collection) as ifile:\n",
    "        MQD = pickle.load(ifile)\n",
    "    \n",
    "    # We show one extra document below the last rank of interest.\n",
    "    cutoff += 1\n",
    "    \n",
    "    n_queries = len(MQD.values()[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(n_queries + 1, 2, figsize=(12, 2 * len(MQD.values()[0])),\n",
    "                             sharex=True, sharey=False)\n",
    "    \n",
    "    ax = axes[0, 0]\n",
    "    \n",
    "    exam_proba = MQD['PBM'][MQD['PBM'].keys()[0]]['model'].exam_proba[:cutoff]\n",
    "\n",
    "    ax.bar(np.arange(len(exam_proba)), exam_proba, 0.75,\n",
    "           alpha=0.7, color='green', align='center')\n",
    "\n",
    "    ax.set_title('PBM Examination Probabilities')\n",
    "    ax.grid(axis='y', which='major')\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    ax.axis('off')\n",
    "\n",
    "    for i, query in enumerate(sorted(MQD['PBM'].keys()), 1):\n",
    "        # Plot left column depicting the attractiveness probabilities\n",
    "        # of documents sorted in decreasing order (No relationship between\n",
    "        # position and document identity should be taken from this plot!).\n",
    "        ax = axes[i, 0]\n",
    "\n",
    "        relevances = sorted(MQD['PBM'][query]['relevances'], reverse=True)[:cutoff]\n",
    "\n",
    "        ax.bar(np.arange(len(relevances)), relevances, 0.75,\n",
    "               alpha=0.7, color='blue', align='center')\n",
    "\n",
    "        ax.set_title('Query: %s (Attractivenesses)' % query)\n",
    "        ax.grid(axis='y', which='major')\n",
    "        \n",
    "        b = 'on' if i + 1 == n_queries else 'off'\n",
    "        lb = 'on' if i + 1 == n_queries else 'off'\n",
    "                \n",
    "        ax.tick_params(axis='both', which='major', left='on', direction='out',\n",
    "                       top='off', right='off', bottom=b, labelleft='on',\n",
    "                       labeltop='off', labelright='off', labelbottom=lb)\n",
    "\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        \n",
    "        ax = axes[i, 1]\n",
    "\n",
    "        exam_proba = MQD['PBM'][query]['model'].exam_proba[:cutoff]\n",
    "        click_proba = relevances * exam_proba\n",
    "        \n",
    "        ax.bar(np.arange(len(click_proba)), click_proba, 0.75,\n",
    "               alpha=0.7, color='red', align='center')\n",
    "\n",
    "        ax.set_title('Atractivenesses * Examination Probabilities')\n",
    "        ax.grid(axis='y', which='major')\n",
    "        \n",
    "        b = 'on' if i + 1 == n_queries else 'off'\n",
    "        lb = 'on' if i + 1 == n_queries else 'off'\n",
    "                \n",
    "        ax.tick_params(axis='both', which='major', left='off', direction='out',\n",
    "                       top='off', right='off', bottom=b, labelleft='off',\n",
    "                       labeltop='off', labelright='off', labelbottom=lb)\n",
    "        \n",
    "        ax.spines['left'].set_visible(False)\n",
    "            \n",
    "    ax.xaxis.set_major_locator(MultipleLocator(1.0))\n",
    "    ax.set_xlim([-0.5, len(relevances) - 0.5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0)\n",
    "    \n",
    "    if output_filename is not None:\n",
    "        fig.savefig(output_filename, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# To save the plots into a file, call:\n",
    "# show_all_queries_relevances('data/60Q/model_query_collection.pkl', output_filename='figures/PBM_60Q_RELEVANCES.png')\n",
    "\n",
    "show_PBM_queries_relevances('data/60Q/model_query_collection.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative Regret Distribution at Specific Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cPickle as pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_regret_filepath(experiment_info_filepath):\n",
    "    return experiment_info_filepath.rstrip('experiment.nfo') + 'regret.npy'\n",
    "\n",
    "\n",
    "def show_algorithms_regret_boxplot(experiments_directory, iteration, click_model_name='PBM'):\n",
    "    # To zero-based.\n",
    "    iteration -= 1\n",
    "    \n",
    "    ranking_model_regrets_at_iteration = defaultdict(list)\n",
    "    \n",
    "    # Go through the given directory and collect all\n",
    "    # experiment *.nfo files beneath it.\n",
    "    for root, dirs, files in os.walk(experiments_directory, topdown=True, followlinks=True):\n",
    "        for fn in files:\n",
    "            if fn.endswith('.nfo'):\n",
    "                fp = os.path.join(root, fn)\n",
    "                with open(fp) as ifile:\n",
    "                    spec = pickle.load(ifile)\n",
    "                    # Filter the desired experiment results.\n",
    "                    if (spec['click_model'].getName() == click_model_name and\n",
    "                        os.path.exists(get_regret_filepath(fp))):\n",
    "                        # Regret of the algorithm in the current experiment.\n",
    "                        regret = np.load(get_regret_filepath(fp))\n",
    "                        # Cumulative regret at the given iteration:\n",
    "                        regret = regret.cumsum()[iteration]\n",
    "                        ranking_model_regrets_at_iteration[spec['ranking_model'].getName()].append(regret)\n",
    "    \n",
    "    # Print the regrets for each ranking model in the experiments directory\n",
    "    # that has NOT been filtered out.\n",
    "#     for ranking_model, regrets in ranking_model_regrets_at_iteration.items():\n",
    "#         print '%s: %r' % (ranking_model, regrets)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    meanprops = dict(marker='D', markeredgecolor='black', markerfacecolor='firebrick')\n",
    "    \n",
    "    flierprops = dict(marker='o', markerfacecolor='green', markersize=6, linestyle='none')\n",
    "\n",
    "    labels, regrets = zip(*ranking_model_regrets_at_iteration.items())\n",
    "    \n",
    "    ax.boxplot(regrets, whis=[5, 95], notch=True, bootstrap=5000,\n",
    "               showmeans=True, meanprops=meanprops, flierprops=flierprops)\n",
    "    \n",
    "    # You can specify a rotation for the tick labels in degrees or with keywords.\n",
    "    plt.xticks(1 + np.arange(len(labels)), labels, rotation=17)\n",
    "    # Pad margins so that markers don't get clipped by the axes\n",
    "    plt.margins(0.2)\n",
    "    # Tweak spacing to prevent clipping of tick-labels\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title('Cumulative Regret Distribution at Iteration %d' % (iteration + 1))\n",
    "\n",
    "show_algorithms_regret_boxplot('experiments/mergerank_benchmark/algorithms/', 10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atractivenesses of documents under PBM and CM models and their parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# The collection of click_models and queries used in the experiments.\n",
    "MQD = np.load('data/60Q/model_query_collection.pkl')\n",
    "\n",
    "# The PBM's intrinsic parameters (examination probability at each rank) is shared\n",
    "# across all the queries in the dataset.\n",
    "pbm_examination_probabilities = MQD['PBM'][MQD['PBM'].keys()[0]]['model'].exam_proba\n",
    "\n",
    "# Mapping \"query id\" -> \"attraction probabilities\"\n",
    "pbm_attraction_probabilities = dict(zip(MQD['PBM'], [MQD['PBM'][q]['relevances'] for q in MQD['PBM']]))\n",
    "cm_attraction_probabilities = dict(zip(MQD['CM'], [MQD['CM'][q]['relevances'] for q in MQD['CM']]))\n",
    "\n",
    "# We use cutoff @5\n",
    "cutoff = 5\n",
    "\n",
    "# Each query has 10 documents.\n",
    "n_documents = 10\n",
    "\n",
    "# Awful, used internally by a user model.\n",
    "identity = np.arange(n_documents, dtype='int32')\n",
    "\n",
    "if not os.path.isdir('parameters'):\n",
    "    os.makedirs('parameters')\n",
    "\n",
    "# Create output file for queries' parameters under PBM click model.\n",
    "for q in MQD['PBM']:\n",
    "    with open('parameters/%s_PBM.txt' % q, 'w') as ofile:\n",
    "        click_model = MQD['PBM'][q]['model']\n",
    "        ideal_ctr = click_model.get_clickthrough_rate(click_model.get_ideal_ranking(cutoff=cutoff),\n",
    "                                                      identity, cutoff=cutoff)\n",
    "        ofile.write('%.6f\\n' % ideal_ctr)\n",
    "        ofile.write(','.join(map(lambda x: '%.6f' % x, click_model.exam_proba)) + '\\n')\n",
    "        ofile.write(','.join(map(lambda x: '%.6f' % x, MQD['PBM'][q]['relevances'])) + '\\n')\n",
    "\n",
    "\n",
    "# Create output file for queries' parameters under PBM click model.\n",
    "for q in MQD['CM']:\n",
    "    with open('parameters/%s_CM.txt' % q, 'w') as ofile:\n",
    "        click_model = MQD['CM'][q]['model']\n",
    "        ideal_ctr = click_model.get_clickthrough_rate(click_model.get_ideal_ranking(cutoff=cutoff),\n",
    "                                                      identity, cutoff=cutoff)\n",
    "        ofile.write('%.6f\\n' % ideal_ctr)\n",
    "        # There are no examination probabilities under CM user model.\n",
    "        # ofile.write(','.join(map(lambda x: '%.6f' % x, click_model.exam_proba)) + '\\n')\n",
    "        ofile.write(','.join(map(lambda x: '%.6f' % x, MQD['CM'][q]['relevances'])) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
